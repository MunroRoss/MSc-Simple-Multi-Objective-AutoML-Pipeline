{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fecc0d2",
   "metadata": {},
   "source": [
    "# MSc Simple Multi-Objective AutoML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33e41f",
   "metadata": {},
   "source": [
    "First I will look at 2 sets of data, 1 binary classification and one multiclass. I will do a complete pipeline and evaluation that will later be automated. That pipeline will include:\n",
    "\n",
    "* Data Preprocessing\n",
    "* Model Training\n",
    "* Hyperparameter optimization.\n",
    "* Model Evaluation with Accuracy\n",
    "* Model Evaluation with Interpretability (specified metrics)\n",
    "* Incorporating into a ranking function for model selection\n",
    "\n",
    "Each automation will be constrained.\n",
    "\n",
    "<img src=\"images\\Pipeline.png\"/>\n",
    "\n",
    "Algorithms that will be used for modelling will be limited to:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Trees\n",
    "3. Random Forest\n",
    "4. LightGBM\n",
    "5. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43ea5f",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "This study will assume that datasets input into the automated pipeline will be reasonably clean and representative, as commonly provided by machine learning dataset repositories. Therefore extensive data cleaning and feature engineering are considered outside the scope of this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed951fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Import the algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8cce9",
   "metadata": {},
   "source": [
    "Telco Customer Churn:\n",
    "\n",
    "https://www.kaggle.com/datasets/blastchar/telco-customer-churn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd3e714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "churn_data = pd.read_csv(\"Datasets/Telco-Customer-Churn.csv\")\n",
    "print(type(churn_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145d80db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7043, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataframe\n",
    "churn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95a6b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   str    \n",
      " 1   gender            7043 non-null   str    \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   str    \n",
      " 4   Dependents        7043 non-null   str    \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   str    \n",
      " 7   MultipleLines     7043 non-null   str    \n",
      " 8   InternetService   7043 non-null   str    \n",
      " 9   OnlineSecurity    7043 non-null   str    \n",
      " 10  OnlineBackup      7043 non-null   str    \n",
      " 11  DeviceProtection  7043 non-null   str    \n",
      " 12  TechSupport       7043 non-null   str    \n",
      " 13  StreamingTV       7043 non-null   str    \n",
      " 14  StreamingMovies   7043 non-null   str    \n",
      " 15  Contract          7043 non-null   str    \n",
      " 16  PaperlessBilling  7043 non-null   str    \n",
      " 17  PaymentMethod     7043 non-null   str    \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7043 non-null   str    \n",
      " 20  Churn             7043 non-null   str    \n",
      "dtypes: float64(1), int64(2), str(18)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Inspect the datatype of each feature and if there are any missing values\n",
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ac4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Munro\\AppData\\Local\\Temp\\ipykernel_30784\\1231571724.py:2: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  churn_data.describe(include='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "      <td>7043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7043</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6531</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>20.2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>3555</td>\n",
       "      <td>3641</td>\n",
       "      <td>4933</td>\n",
       "      <td>6361</td>\n",
       "      <td>3390</td>\n",
       "      <td>3096</td>\n",
       "      <td>3498</td>\n",
       "      <td>3088</td>\n",
       "      <td>3095</td>\n",
       "      <td>3473</td>\n",
       "      <td>2810</td>\n",
       "      <td>2785</td>\n",
       "      <td>3875</td>\n",
       "      <td>4171</td>\n",
       "      <td>2365</td>\n",
       "      <td>11</td>\n",
       "      <td>5174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        customerID gender Partner Dependents PhoneService MultipleLines  \\\n",
       "count         7043   7043    7043       7043         7043          7043   \n",
       "unique        7043      2       2          2            2             3   \n",
       "top     7590-VHVEG   Male      No         No          Yes            No   \n",
       "freq             1   3555    3641       4933         6361          3390   \n",
       "\n",
       "       InternetService OnlineSecurity OnlineBackup DeviceProtection  \\\n",
       "count             7043           7043         7043             7043   \n",
       "unique               3              3            3                3   \n",
       "top        Fiber optic             No           No               No   \n",
       "freq              3096           3498         3088             3095   \n",
       "\n",
       "       TechSupport StreamingTV StreamingMovies        Contract  \\\n",
       "count         7043        7043            7043            7043   \n",
       "unique           3           3               3               3   \n",
       "top             No          No              No  Month-to-month   \n",
       "freq          3473        2810            2785            3875   \n",
       "\n",
       "       PaperlessBilling     PaymentMethod TotalCharges Churn  \n",
       "count              7043              7043         7043  7043  \n",
       "unique                2                 4         6531     2  \n",
       "top                 Yes  Electronic check         20.2    No  \n",
       "freq               4171              2365           11  5174  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "churn_data.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff0696c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87be3a",
   "metadata": {},
   "source": [
    "Quick things to realise when inspecting the data:\n",
    "\n",
    "* The ID column will be useless in modelling and will need to be dropped consistently accross datasets, it can be seen that the **number of unique values = number of rows** therefore some logic may be implemented to decide when to drop.\n",
    "\n",
    "* The column \"TotalCharges\" has 6531 unique values, after inspection, these are numerical charge values stored as strings, in the dtype object. it would be good to process these in the pipeline.\n",
    "\n",
    "To maintain a simple and generic preprocessing pipeline, features with type object can be treated as categorical variables. It can be implemented that features with more than 20 distinct values can be dropped, as this represents high-cardinality and can negatively impact interpretability and explanation stability. No attempt will be made to coerce these features to numeric values, as this would require dataset specific cleaning logic and would defeat the point of implementing this across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124af77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function for identifying data types of each column for preprocessing, splitting them into groups\n",
    "\n",
    "def get_feature_groups(data):\n",
    "\n",
    "    categorical_columns = data.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns #detect all categorical column labels\n",
    "    numeric_columns = data.select_dtypes(include=\"number\").columns #detect all numerical column labels \n",
    "    bool_columns = data.select_dtypes(include=\"bool\").columns #detect possible boolean columns\n",
    "\n",
    "    numeric_columns = numeric_columns.union(bool_columns).tolist() #merge boolean in with numeric as they are 0 and 1 representations\n",
    "\n",
    "    high_cardinality = data[categorical_columns].nunique() > 20 #isolate high cardinality features\n",
    "\n",
    "    #columns_to_drop = high_cardinality[high_cardinality].index.tolist() #create a column to drop list\n",
    "    categorical_columns = high_cardinality[~high_cardinality].index.tolist() #dropping high cardinality features from categorical list\n",
    "    \n",
    "    return numeric_columns, categorical_columns, #columns_to_drop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b081f7",
   "metadata": {},
   "source": [
    "By creating a list for each type of variable, this will lead into transforming the columns and preparing for a scikit learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927e2c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5634, 20), (5634,), (1409, 20), (1409,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "y = churn_data[\"Churn\"]\n",
    "X = churn_data.drop(\"Churn\", axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053d38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns, categorical_columns = get_feature_groups(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06012077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SeniorCitizen', 'tenure', 'MonthlyCharges']\n",
      "['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n"
     ]
    }
   ],
   "source": [
    "print(numerical_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df1fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column transformer\n",
    "\n",
    "preprocesser = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_columns),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_columns)\n",
    "    ],\n",
    "    remainder = \"drop\",\n",
    "    n_jobs=-1 # enable use of all cpu threads locally\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92b1ce",
   "metadata": {},
   "source": [
    "Column transformer will help to transform the numerical features and the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fbf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of models to iterate over\n",
    "\n",
    "models = {\n",
    "    \"logreg\": LogisticRegression(),\n",
    "    \"dt\": DecisionTreeClassifier(),\n",
    "    \"rf\": RandomForestClassifier(),\n",
    "    \"svm\": LinearSVC(),\n",
    "    \"lgb\": LGBMClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3f7a288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 4138\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 414\n",
      "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265531 -> initscore=-1.017418\n",
      "[LightGBM] [Info] Start training from score -1.017418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create an iterative pipeline\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocess\", preprocesser),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "\n",
    "    results[name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77ffe5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg': 0.8211497515968772, 'dt': 0.7139815471965933, 'rf': 0.78708303761533, 'svm': 0.8183108587650816, 'lgb': 0.7984386089425124}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ed520",
   "metadata": {},
   "source": [
    "While the previous pipeline it clean, if cross validation is used feature selection based on X_train will cause data leakage for each fold as the validation set would have been used to make transformations on the training data. Hence we want to implement feature selection and preprocessing of data for each training fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93832a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5634, 20), (5634,), (1409, 20), (1409,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(churn_data[\"Churn\"]) # encode category labels\n",
    "X = churn_data.drop(\"Churn\", axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "610f1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer class to group features in first step of a pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GroupFeatures(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # Constructor method\n",
    "    def __init__(self, max_unique=20):\n",
    "        self.max_unique = max_unique\n",
    "\n",
    "    #Fit method learns column groupings and detects high cardinality --> metadata for future steps\n",
    "    def fit(self, X, y=None):\n",
    "        categorical = X.select_dtypes(include=[\"object\", \"string\", \"category\"])\n",
    "        numeric = X.select_dtypes(include=\"number\")\n",
    "        boolean = X.select_dtypes(include=\"bool\")\n",
    "\n",
    "        high_cardinality = categorical.nunique() > self.max_unique #Detects high cardinality above maximum value specified\n",
    "\n",
    "        self.numeric_columns_ = numeric.columns.union(boolean.columns).tolist() #Creates numerical column list\n",
    "        self.categorical_columns_ = high_cardinality[~high_cardinality].index.tolist() #Creates categorical column list - high cardinality features\n",
    "\n",
    "        self.keep_columns_ = self.numeric_columns_ + self.categorical_columns_\n",
    "\n",
    "        return self\n",
    "\n",
    "    # passes through the data untransformed \n",
    "    def transform(self, X):\n",
    "        return X[self.keep_columns_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f65d6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer for feature preprocessing\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), make_column_selector(dtype_include=\"number\")), \n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include=[\"object\", \"string\", \"category\"])),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e434b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1197, number of negative: 3310\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 414\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265587 -> initscore=-1.017130\n",
      "[LightGBM] [Info] Start training from score -1.017130\n",
      "[LightGBM] [Info] Number of positive: 1197, number of negative: 3310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 414\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265587 -> initscore=-1.017130\n",
      "[LightGBM] [Info] Start training from score -1.017130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1197, number of negative: 3310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 414\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265587 -> initscore=-1.017130\n",
      "[LightGBM] [Info] Start training from score -1.017130\n",
      "[LightGBM] [Info] Number of positive: 1196, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 414\n",
      "[LightGBM] [Info] Number of data points in the train set: 4507, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265365 -> initscore=-1.018268\n",
      "[LightGBM] [Info] Start training from score -1.018268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1197, number of negative: 3311\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 414\n",
      "[LightGBM] [Info] Number of data points in the train set: 4508, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265528 -> initscore=-1.017432\n",
      "[LightGBM] [Info] Start training from score -1.017432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Implement Cross-Validation Evaluation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"roc_auc\": \"roc_auc\"\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    # Create pipeline that include GroupFeatures in first step\n",
    "    pipeline = Pipeline([\n",
    "    (\"features\", GroupFeatures(max_unique=20)),\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "    pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    return_estimator=True\n",
    "    )\n",
    "\n",
    "    results[name] = cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051f17ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78881988, 0.81011535, 0.78615794, 0.80834073, 0.79928952])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['logreg']['test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58433b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.0511601 , 0.04904747, 0.04824543, 0.04755926, 0.04179358]),\n",
       " 'score_time': array([0.02668858, 0.02503681, 0.02410722, 0.0256021 , 0.02678967]),\n",
       " 'estimator': [Pipeline(steps=[('features', GroupFeatures()),\n",
       "                  ('preprocess',\n",
       "                   ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x0000020869246ED0>),\n",
       "                                                   ('cat',\n",
       "                                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x000002086DB7BED0>)])),\n",
       "                  ('model', LogisticRegression())]),\n",
       "  Pipeline(steps=[('features', GroupFeatures()),\n",
       "                  ('preprocess',\n",
       "                   ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x000002086E3D2690>),\n",
       "                                                   ('cat',\n",
       "                                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x00000208701A4090>)])),\n",
       "                  ('model', LogisticRegression())]),\n",
       "  Pipeline(steps=[('features', GroupFeatures()),\n",
       "                  ('preprocess',\n",
       "                   ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x000002086DBB6D90>),\n",
       "                                                   ('cat',\n",
       "                                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x00000208701C3450>)])),\n",
       "                  ('model', LogisticRegression())]),\n",
       "  Pipeline(steps=[('features', GroupFeatures()),\n",
       "                  ('preprocess',\n",
       "                   ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x0000020870204350>),\n",
       "                                                   ('cat',\n",
       "                                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x0000020870208FD0>)])),\n",
       "                  ('model', LogisticRegression())]),\n",
       "  Pipeline(steps=[('features', GroupFeatures()),\n",
       "                  ('preprocess',\n",
       "                   ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x000002086FE59010>),\n",
       "                                                   ('cat',\n",
       "                                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                    <sklearn.compose._column_transformer.make_column_selector object at 0x000002086FF8A8D0>)])),\n",
       "                  ('model', LogisticRegression())])],\n",
       " 'test_accuracy': array([0.78881988, 0.81011535, 0.78615794, 0.80834073, 0.79928952]),\n",
       " 'test_f1': array([0.55925926, 0.60805861, 0.57345133, 0.59398496, 0.57992565]),\n",
       " 'test_roc_auc': array([0.81841444, 0.84237919, 0.83915992, 0.84112253, 0.85117259])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['logreg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca38937",
   "metadata": {},
   "source": [
    "`RandomSearchCV()` will be used for hyperparameter optimisation and a parameter space needs to be defined, for example:\n",
    "\n",
    "Logistic Regression → C, penalty\n",
    "\n",
    "Decision Tree → max_depth, min_samples_leaf\n",
    "\n",
    "Random Forest → n_estimators, max_depth\n",
    "\n",
    "XGBoost/LightGBM → max_depth, learning_rate, n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efe6fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define param distributions\n",
    "\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "param_grids = {\n",
    "    \"logreg\" : [ #Not all solvers are compatible with each type of regularization. so an list of dictionaries is implemented to enforce compatibility.\n",
    "        {   # liblinear: supports l1, l2\n",
    "            \"model__solver\": [\"liblinear\"],\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__C\": loguniform(1e-4, 1e2),\n",
    "            \"model__class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        {   # lbfgs: supports l2\n",
    "            \"model__solver\": [\"lbfgs\"],\n",
    "            \"model__penalty\": [\"l2\"],\n",
    "            \"model__C\": loguniform(1e-4, 1e2),\n",
    "            \"model__class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        {   # saga: supports elasticnet (needs l1_ratio)\n",
    "            \"model__solver\": [\"saga\"],\n",
    "            \"model__penalty\": [\"elasticnet\"],\n",
    "            \"model__C\": loguniform(1e-4, 1e2),\n",
    "            \"model__l1_ratio\": [0.1, 0.5, 0.9], #only used for elasticnet\n",
    "            \"model__class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        {   # saga can do l1 / l2 as well\n",
    "            \"model__solver\": [\"saga\"],\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__C\": loguniform(1e-4, 1e2),\n",
    "            \"model__class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        ],\n",
    "\n",
    "    \"dt\" : {\n",
    "        \"model__max_depth\": randint(1, 20), \n",
    "        \"model__min_samples_split\": randint(2, 20), \n",
    "        \"model__min_samples_leaf\": randint(1, 20),\n",
    "        \"model__class_weight\": [None, \"balanced\"],\n",
    "    },\n",
    "    \"rf\" : {\n",
    "        \"model__n_estimators\": randint(100, 500),\n",
    "        \"model__max_depth\": randint(1, 20),\n",
    "        \"model__min_samples_split\": randint(2, 20),\n",
    "        \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "    },\n",
    "    \"svm\" : [\n",
    "        {\n",
    "            \"model__penalty\": [\"l2\"],\n",
    "            \"model__loss\": [\"squared_hinge\", \"hinge\"],\n",
    "            \"model__C\": loguniform(1e-4, 1e2),\n",
    "            \"model__class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        { #L1 only works with square_hinge and model_dual = False --> model_dual is defaulted as \"auto\"\n",
    "            \"model__penalty\": [\"l1\"],\n",
    "            \"model__loss\": [\"squared_hinge\"],\n",
    "            \"model__C\": loguniform(1e-4, 1e2),\n",
    "            \"model__class_weight\": [None, \"balanced\"],\n",
    "        },\n",
    "        ],\n",
    "    \"lgb\" : {\n",
    "        \"model__num_leaves\": randint(31, 60),       # controls tree complexity\n",
    "        \"model__max_depth\": randint(5, 15),         # limits tree depth\n",
    "        \"model__learning_rate\": loguniform(1e-3, 0.1), # step size\n",
    "        \"model__n_estimators\": randint(100, 500),   # number of boosting rounds\n",
    "        \"model__subsample\": uniform(0.7, 0.3),      # row sampling\n",
    "        \"model__colsample_bytree\": uniform(0.7, 0.3), # feature sampling\n",
    "        \"model__class_weight\": [None, \"balanced\"], # to handle class imbalance\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2adf5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "MODEL: logreg\n",
      "==============================\n",
      "Best Params: {'model__C': np.float64(0.39079671568228835), 'model__class_weight': None, 'model__penalty': 'l2', 'model__solver': 'saga'}\n",
      "Best ROC-AUC:  0.8385\n",
      "Best Accuracy: 0.7984\n",
      "Best F1:       0.5824\n",
      "\n",
      "==============================\n",
      "MODEL: dt\n",
      "==============================\n",
      "Best Params: {'model__class_weight': None, 'model__max_depth': 4, 'model__min_samples_leaf': 8, 'model__min_samples_split': 4}\n",
      "Best ROC-AUC:  0.8231\n",
      "Best Accuracy: 0.7865\n",
      "Best F1:       0.5170\n",
      "\n",
      "==============================\n",
      "MODEL: rf\n",
      "==============================\n",
      "Best Params: {'model__class_weight': None, 'model__max_depth': 7, 'model__min_samples_split': 12, 'model__n_estimators': 187}\n",
      "Best ROC-AUC:  0.8380\n",
      "Best Accuracy: 0.7984\n",
      "Best F1:       0.5558\n",
      "\n",
      "==============================\n",
      "MODEL: svm\n",
      "==============================\n",
      "Best Params: {'model__C': np.float64(0.39079671568228835), 'model__class_weight': None, 'model__loss': 'squared_hinge', 'model__penalty': 'l1'}\n",
      "Best ROC-AUC:  0.8370\n",
      "Best Accuracy: 0.7985\n",
      "Best F1:       0.5780\n",
      "[LightGBM] [Info] Number of positive: 1496, number of negative: 4138\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 414\n",
      "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265531 -> initscore=-1.017418\n",
      "[LightGBM] [Info] Start training from score -1.017418\n",
      "\n",
      "==============================\n",
      "MODEL: lgb\n",
      "==============================\n",
      "Best Params: {'model__class_weight': None, 'model__colsample_bytree': np.float64(0.7299924747454009), 'model__learning_rate': np.float64(0.008288916866885139), 'model__max_depth': 9, 'model__n_estimators': 199, 'model__num_leaves': 38, 'model__subsample': np.float64(0.9124217733388136)}\n",
      "Best ROC-AUC:  0.8408\n",
      "Best Accuracy: 0.7973\n",
      "Best F1:       0.5311\n"
     ]
    }
   ],
   "source": [
    "# Implement RandomSearchCV test wutg 5 iterations\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"roc_auc\": \"roc_auc\"\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    # Create pipeline that include GroupFeatures in first step\n",
    "    pipeline = Pipeline([\n",
    "    (\"features\", GroupFeatures(max_unique=20)),\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    clf = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=5,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        refit=\"roc_auc\" #Refit an estimator using the best found parameters on the whole dataset using one main metric\n",
    "    )\n",
    "\n",
    "    search = clf.fit(X_train, y_train)\n",
    "\n",
    "    results[name] = search\n",
    "\n",
    "    best_idx = search.best_index_\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"MODEL: {name}\")\n",
    "    print(\"==============================\")\n",
    "    print(\"Best Params:\", search.best_params_)\n",
    "    print(f\"Best ROC-AUC:  {search.cv_results_['mean_test_roc_auc'][best_idx]:.4f}\")\n",
    "    print(f\"Best Accuracy: {search.cv_results_['mean_test_accuracy'][best_idx]:.4f}\")\n",
    "    print(f\"Best F1:       {search.cv_results_['mean_test_f1'][best_idx]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27551897",
   "metadata": {},
   "source": [
    "Automatic training and output of each models metrics have been completer, now onto shap values and model ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce022a22",
   "metadata": {},
   "source": [
    "## SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08d27829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Munro\\anaconda3\\envs\\msc_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the best logreg model trained on full test set with best parameters\n",
    "best_logreg_pipeline = results[\"logreg\"].best_estimator_\n",
    "\n",
    "# the transformed data\n",
    "X_trans = best_logreg_pipeline.named_steps[\"preprocess\"].transform(X_train)\n",
    "\n",
    "# The final fitted model\n",
    "logreg_model = best_logreg_pipeline.named_steps[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ccfbac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.37749204e-01 -4.65683364e-01 -4.73723375e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00]\n",
      " [-4.37749204e-01  8.85536787e-01  1.07475386e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-4.37749204e-01 -1.28460467e+00 -1.37649913e+00 ...  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [-4.37749204e-01 -8.34197950e-01 -1.45294499e+00 ...  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00]\n",
      " [ 2.28441306e+00 -8.34197950e-01  1.14953785e+00 ...  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00]\n",
      " [-4.37749204e-01 -2.60953038e-01 -1.49781538e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df62068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=np.float64(0.39079671568228835), penalty='l2',\n",
      "                   solver='saga')\n"
     ]
    }
   ],
   "source": [
    "print(logreg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1907a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', GroupFeatures()),\n",
      "                ('preprocess',\n",
      "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000002086FE1D210>),\n",
      "                                                 ('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000002087045DC90>)])),\n",
      "                ('model',\n",
      "                 LogisticRegression(C=np.float64(0.39079671568228835),\n",
      "                                    penalty='l2', solver='saga'))])\n"
     ]
    }
   ],
   "source": [
    "print(best_logreg_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "442bba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8204400283889283 0.6328011611030478 0.7449279037750887\n"
     ]
    }
   ],
   "source": [
    "logreg_pred = best_logreg_pipeline.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "acc = accuracy_score(y_test, logreg_pred)\n",
    "f1 = f1_score(y_test, logreg_pred)\n",
    "auc = roc_auc_score(y_test, logreg_pred)\n",
    "\n",
    "print(acc, f1, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2c1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
